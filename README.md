# Predicting-GrainYield
Comparing the performance of classification algorithms for predicting “GrainYield” class.

# Abstract
This study systematically evaluates the performance of multiple classification algorithms on imbalanced datasets within the context of grain yield prediction. A comprehensive methodology was adopted, encompassing data preprocessing techniques such as K-Nearest Neighbors (KNN) based missing value imputation and MinMaxScaler normalization. To address class imbalance, the GrainYield feature is converted into binary values: 'A' is assigned 1, while 'B' and 'C' are assigned 0, due to the significant difference between the total number of 'A' values and the combined total of 'B' and 'C' values. Also, advanced oversampling method Synthetic Minority Oversampling Technique (SMOTE) were implemented. Feature selection through Recursive Feature Elimination with Cross-Validation (RFECV) ensured the retention of the most informative features. Classification models, including Naive Bayes (NB), Decision Tree, Random Forest, Gradient Boosting, Logistic Regression, Support Vector Machines (SVM), K-Nearest Neighbors, Adaptive Boosting (AdaBoost), Light Gradient Boosting Machine (LightGBM), and Extra Tree were evaluated using metrics like classification accuracy (CA), area under the ROC curve (AUC), precision, recall, and F1 score. Results revealed Extra Tree as top performers, demonstrating significant improvements in classification performance with the use of oversampling techniques. These findings underscore the importance of robust preprocessing and resampling strategies in addressing class imbalance and optimizing classification outcomes.

# Methodology

![image](https://github.com/user-attachments/assets/0f302457-74a3-401c-9b85-83e25b973d81)

# Data Preprocessing 
The dataset used in this study underwent a series of preprocessing steps to ensure its suitability for classification analysis and to improve the accuracy of the models. To address the issue of missing data, the KNN algorithm was applied for imputation. This method estimates missing values based on the similarity between data points, ensuring that the dataset's integrity is preserved and minimizing the potential for bias caused by incomplete information. Additionally, the dataset was normalized using the MinMaxScaler technique, which transformed all numerical features to a range between 0 and 1. This step standardized the feature scales, ensuring consistent representation across all features and reducing the risk of bias that could arise from differing magnitudes. These preprocessing techniques were essential for optimizing the dataset for classification and ensuring that the subsequent analysis would be both accurate and reliable.

# Balancing the Dataset
To address the class imbalance issues in the dataset, The technique of converting features into binary variables was used. Within this technique GrainYield feature is converted into binary values: 'A' is assigned 1, while 'B' and 'C' are assigned 0, due to the significant difference between the total number of 'A' values and the combined total of 'B' and 'C' values. Also, SMOTE was used to generate synthetic samples for the minority class by interpolating between existing data points, thus improving its representation. Together, these techniques addressed the class imbalance and contributed to a more robust and effective model.

# Feature Selection 
Feature selection in this study was focused on reducing dimensionality while retaining the essential information needed for effective classification. To achieve this, Recursive Feature Elimination with Cross-Validation (RFECV) was used to systematically eliminate less informative features. This method assessed the importance of each feature and selected only those with the highest predictive value across models, ensuring that the final set of features was optimized for performance. The feature selection technique helped streamline the dataset while maintaining its relevance and predictive power.

# Classification Algorithms
The processed dataset was used to train and evaluate a variety of classification algorithms, each offering unique advantages depending on the nature of the data. NB classifier, based on Bayes' theorem, was chosen for its probabilistic approach, making it well-suited for datasets where features follow a Gaussian distribution. The Decision Tree algorithm, a tree-structured classifier, splits the data into subsets based on the most significant features, providing both interpretability and computational efficiency. Random Forest, an ensemble method that leverages multiple decision trees, was used to enhance accuracy and robustness, particularly in handling complex, non-linear patterns. Gradient Boosting, another tree-based ensemble technique, was employed to optimize model performance by iteratively reducing errors, making it effective for complex datasets. Logistic Regression, a baseline algorithm for binary classification, was included for its simplicity and interpretability. SVM were used for both linearly and non-linearly separable data, requiring careful tuning of kernels and hyperparameters to achieve optimal performance. KNN, a non-parametric algorithm, classifies samples based on the majority vote of their nearest neighbors, but is highly sensitive to feature scaling. AdaBoost, an ensemble method combining weak learners to form a strong classifier, focuses on improving the model’s performance on difficult-to-classify samples. LightGBM, a gradient boosting framework designed for speed and efficiency, was selected for its ability to handle large datasets and complex feature interactions. Lastly, Extra Trees, an ensemble method similar to Random Forest, was used to increase diversity among trees by adding additional randomness to the feature splits. These algorithms were evaluated for their suitability in handling the processed dataset and their ability to deliver optimal classification performance.

# Model Evaluation 
The evaluation of the trained models was conducted using a consistent and comprehensive set of metrics to ensure a robust assessment of their performance. Key performance metrics, including CA, AUC, precision, recall, and F1 score, were systematically calculated for each model. These metrics provided insights into various aspects of model effectiveness, such as their ability to correctly classify instances, balance precision and recall, and perform well across different decision thresholds. In addition to numerical evaluations, graphical analyses were performed using ROC curves and confusion matrices. These visual tools facilitated the identification of classification performance trends and misclassification patterns, offering a deeper understanding of the strengths and weaknesses of each model. Through this detailed evaluation process, the classifier that demonstrated the highest overall performance across all metrics and visualization analyses was identified as the most effective, highlighting its superiority in addressing the classification task at hand.

# Data
In this study, data set that contains several attributes about information collected during an agricultural experiment aimed at predicting grain yield. It includes attributes capturing environmental, procedural, and demographic variables. Each record represents a specific instance of grain cultivation, with the variables categorized as textual or numerical data. The primary target variable, ‘GrainYield’, represents the mean yield derived from measurements at three crop-cut spots. A summary of the variables is provided in figure: 

![image](https://github.com/user-attachments/assets/8c2aafda-0c83-4ca7-a717-13e89664fe34)

After applying a feature selection method, 10 different classification methods were implemented on the selected features with RFECV. Following the implementation of these classification algorithms, the AUC and CA values were calculated and sorted in decreasing order of CA, as shown in figure: 

![image](https://github.com/user-attachments/assets/b7641860-13bf-4aea-90f2-63a49eac54cd)

The corresponding ROC curves:

![image](https://github.com/user-attachments/assets/3aa8e7ca-459f-42af-8c07-99ceed3f205e)

As shown in CA and AUC's figure, Extra Tree achieves the highest classification accuracy. Additionally, as illustrated in figure of ROC Curves, it exhibits a well-shaped ROC curve. Therefore, Extra Tree classification method is appropriate to predict ‘GrainYield’. The confusion matrix for the selected Extra Tree algorithm is:

![image](https://github.com/user-attachments/assets/dbe88a0f-cf94-485c-9d1e-ac23da14ef6c)

As shown in the confusion matrix, the true positive and true negative values are significantly larger than the false negative and false positive values, indicating that the Extra Trees classification algorithm performs well.
